Parameter scaling effects Comparing small and large versions within model families
reveals inconsistent scaling benefits. While Mistral 123B (ğ‘šğ‘‘ = 1.036) significantly
outperforms Mistral 7B (ğ‘šğ‘‘ = 3.487), Qwen2.5 7B (ğ‘šğ‘‘ = 0.817) shows better alignment
than its larger counterpart Qwen2.5 72B (ğ‘šğ‘‘ = 1.143). It suggests that parameter count
alone does not guarantee improved cultural representation.