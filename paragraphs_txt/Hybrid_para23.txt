A decision tree essentially classifies the training data into sets. These sets are formed by  branching on the attribute values that the examples in the training data.  A naïve decision tree would sequentially branch on all attributes.  Techniques to handle problems with the naïve decision tree are discussed in Section 5. A perfect decision tree would be structured such that all the training examples at a node in the tree are all of the same classification.  But this rarely happens since there would always be a few outliers due to noise.  The prediction at the node is then the majority of the classification of the various examples with biasing incorporated if required. When the formation of the tree is completed, prediction can take place.  Given a piece of data, we traverse a path from the root to the leaf of the tree by taking edges corresponding to the value that the data depicts for the attribute at the node.  The prediction of the leaf node is then the prediction that is returned.