Decision Trees classifiers, unless pruned, even for a small number of attributes require prohibitively large memory and running times. Pruning the tree by using moderate-to-large Ï‡-values gets around this problem, but introduces another one - quite often a large number of training examples get cluttered into the same leaf node, because no matter which attribute one chooses, the information gain is never sufficient to split the node. Such leaves quite often have training instances of many different classes and therefore end up misclassifying test examples of all classes except the majority class. What is needed is a more ``intelligent'' inference mechanism at such nodes, that does a better job at classifying than just assigning the class with the maximum number of instances.