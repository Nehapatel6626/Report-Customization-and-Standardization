In this paper, we implement and examine the naïve Bayes approach along with the decision tree approach.  In order to study the effects of boosting, cross-validation and ensemble construction, we chose decision trees and implemented these techniques for them.  A novel hybrid approach to learning using decision trees and naïve Bayes is proposed.  As shown in Section 6, the hybrid approach displays a spectrum of solutions to learning.  At one end of the spectrum lies the naïve Bayes method, while at the other end lies the decision tree approach. In other parts of the spectrum, the two techniques are combined by learning a decision tree using pruning methods, and including a Naïve Bayes model at the leaf nodes, consisting of the remaining attributes that have not been branched upon.  This method takes advantage of the speed of the naïve Bayes approach, and uses the decision tree in order to break as many dependencies as possible in order to suit the data to the naïve Bayes assumption of attribute independence.  Pruning on a decision tree is done due to memory concerns, and to avoid over-fitting.  If the amount of data to be learnt, and the number of attributes are large, then pruning is required to keep the tree at a manageable size.  This would result in the loss of data. In such a case, the use of a naïve Bayes model at the leaf nodes would allow for better predictions by recovering some of the lost data. It would be fast and not memory consuming, thus leading to an overall good predictor.