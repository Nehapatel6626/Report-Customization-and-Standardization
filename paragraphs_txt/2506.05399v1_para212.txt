Figure 5: Attention mechanism: (left) scaled dot-product attention, (right) multi-head attention.