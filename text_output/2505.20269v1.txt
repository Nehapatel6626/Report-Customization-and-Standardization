5
2
0
2

y
a
M
6
2

]

O
L
.
s
c
[

1
v
9
6
2
0
2
.
5
0
5
2
:
v
i
X
r
a

Comparing Neural Network Encodings for
Logic-based Explainability

Levi Cordeiro Carvalho, Saulo A. F. Oliveira, and Thiago Alves Rocha (cid:0)

Instituto Federal do Ceará (IFCE), Brazil
levi.carvalho@ifce.edu.br
saulo.oliveira@ifce.edu.br
thiago.alves@ifce.edu.br (cid:0)

Abstract. Providing explanations for the outputs of artificial neural
networks (ANNs) is crucial in many contexts, such as critical systems,
data protection laws and handling adversarial examples. Logic-based
methods can offer explanations with correctness guarantees, but face
scalability challenges. Due to these issues, it is necessary to compare
different encodings of ANNs into logical constraints, which are used in
logic-based explainability. This work compares two encodings of ANNs:
one has been used in the literature to provide explanations, while the
other will be adapted for our context of explainability. Additionally, the
second encoding uses fewer variables and constraints, thus, potentially
enhancing efficiency. Experiments showed similar running times for
computing explanations, but the adapted encoding performed up to 18%
better in building logical constraints and up to 16% better in overall time.

Keywords: Artificial Neural Networks
Intelligence · Logic-based Explainable AI.

· Explainable Artificial

1

Introduction

Artificial neural networks (ANNs) are widely applied in tasks like computer
vision, speech recognition, and pattern recognition [13]. Despite their success,
ANNs are often considered black-box algorithms. Such a lack of interpretability
poses risks in critical domains such as medical and financial applications,
where understanding model decisions is crucial. Additionally, the presence of
adversarial examples highlights the need for explainability in machine learning
algorithms, including neural networks. An adversarial example is an instance
misclassified by a machine learning model and also slightly different from another
correctly classified instance [6].

This preprint has not undergone peer review or any post-submission improvements
or corrections. The Version of Record of this contribution is published in Intelligent
Systems, LNCS, vol 15412 and is available online at https://doi.org/10.1007/
978-3-031-79029-4_20.

 
 
 
 
 
 
2

L. Carvalho, S. Oliveira and T. Rocha

In this work, an explanation for a prediction made by an ANN is a subset
of features and their values that alone suffice for the prediction. If an instance
has the features in this subset, the ANN makes the same prediction, regardless
of the values of other features. For example, given an instance {sneeze = T rue,
weight = 70 kg, headache = T rue, age = 40 years} and its ANN output flu,
a possible explanation could be {sneeze = T rue, headache = T rue}. That is, if
an instance has the features sneeze = T rue and headache = T rue, the ANN
prediction is flu, regardless of weight and age values. A minimal explanation
avoids redundancy by including only essential information. An explanation is
considered minimal when removing any feature results in the loss of assurance
that every instance satisfying the explanation maintains the same output. Then,
a minimal explanation avoids redundancy, providing only essential information.
Heuristic methods, such as ANCHOR [15] and LIME [14], have been used to
provide explanations for machine learning models. However, these approaches
explore the instance space locally, not resulting in explanations that have
minimal sizes and formal guarantees of correctness. Correctness guarantees
are provided when there are no instances with the values specified in the
explanation such that the ANN makes a different prediction. Moreover, minimal
explanations are desired since they do not contain redundancy, making them
easier to understand and interpret.

Some approaches aim to provide explanations for machine learning models
with formal guarantees of correctness [16, 8, 4, 7, 1, 18]. Ignatiev et al. [8]
proposed a logic-based algorithm that gives minimal and correct explanations
for ANNs, utilizing logical constraints originally designed for finding adversarial
examples Fischetti and Jo [5]. These constraints include linear equations,
implications, solved using a Mixed Integer Linear
inequalities, and logical
Programming (MILP) solver. However, scalability issues arise, particularly with
large ANNs, necessitating further development before deployment in large-scale
production environments.

This work explores two different encodings to improve the scalability of
providing correct minimal explanations for ANNs, building upon [8]. In addition
to the logical constraints of [5], we adopt the encoding proposed by Tjeng et al.
[17], which uses fewer variables and constraints, and excludes logical implications.
By reducing variables and constraints compared to [5], our approach aims to
enhance explanation computation performance. To adapt the approach of [17]
for explanations, we introduce new constraints to ensure correctness. In line
with the encodings proposed by Fischetti and Jo [5] and Tjeng et al. [17], we
also compute lower and upper bounds for each neuron. These bounds are found
through optimization using a MILP solver. Moreover, these bounds can aid the
solver in computing explanations more rapidly. In this manner, we compare the
time required for constructing logical constraints with lower and upper bounds
of each neuron, along with the time needed for computing explanations.

We conducted experiments to evaluate both encodings. Our adaptation of the
encoding proposed in [17] exhibits a better running time in building encodings
for ANNs with two layers and tens of neurons, showing an improvement of up

Comparing Neural Network Encodings for Logic-based Explainability

3

to 18%. Surprisingly, both methods exhibit similar running times for computing
explanations. Furthermore, our adaptation outperforms the other encoding in
the overall time, encompassing both building logical constraints and computing
explanations. In this case, the results indicate an improvement of up to 16%. In
summary, our main contributions are described in the following:

– Adaptation of the encoding proposed in [17] to provide explanations for
ANNs; additional constraints were incorporated to address the problem of
computing explanations.

– Comparative analysis of the running time for building the logical constraints
between the two approaches. Additionally, we analyze the time for generating
explanations using both encodings.

– Publicly available

implementations

of both encodings

for finding

explanations for ANNs1.

In the next section, we review some concepts and terminologies about Logic,
MILP and ANNs. Sections 3 and 4 show how to compute explanations with
and without implications, respectively. Section 4 describes our adaptation of the
encoding proposed in [17]. Experiments and results are presented in Section 5.
Finally, conclusions and future work are described in Section 6.

2 Background

In this section, we introduce some initial concepts and terminology to understand
the rest of this work.

2.1 First-order Logic over LRA

In this work, we use first-order logic (FOL) to give explanations with guarantees
of correctness. We use quantifier-free first-order formulas over the theory of linear
real arithmetic (LRA). Then, first-order variables are allowed to take values from
the real numbers R. For details, see [11]. Therefore, we consider formulas as
defined below:

F, G := s | (F ∧ G) | (F ∨ G) | (¬F ) | (F → G),
n
(cid:88)

n
(cid:88)

wixi ≤ b |

wixi < b,

p :=

(1)

i=1

i=1

such that F and G are quantifier-free first-order formulas over the theory of linear
real arithmetic. Moreover, p represents the atomic formulas such that n ≥ 1, each
wi and b are fixed real numbers, and each xi is a first-order variable. Observe
that we allow the use of other letters for variables instead of xi, such as si, zi,
qi. For example, (2.5x1 + 3.1x2 ≥ 6) ∧ (x1 = 1 ∨ x1 = 2) ∧ (x1 = 2 → x2 ≤ 1.1)

1 https://github.com/LeviCC8/Explications-ANNs

4

L. Carvalho, S. Oliveira and T. Rocha

is a formula by this definition. Observe that we allow standard abbreviations as
¬(2.5x1 + 3.1x2 < 6) for 2.5x1 + 3.1x2 ≥ 6.

Since we are assuming the semantics of formulas over the domain of real
numbers, an assignment A for a formula F is a mapping from the first-order
variables of F to elements in the domain of real numbers. For instance, {x1 (cid:55)→
2.3, x2 (cid:55)→ 1} is an assignment for (2.5x1 + 3.1x2 ≥ 6) ∧ (x1 = 1 ∨ x1 = 2) ∧ (x1 =
2 → x2 ≤ 1.1). An assignment A satisfies a formula F if F is true under this
assignment. For example, {x1 (cid:55)→ 2, x2 (cid:55)→ 1.05} satisfies the formula in the above
example, whereas {x1 (cid:55)→ 2.3, x2 (cid:55)→ 1} does not satisfy it.

A formula F is satisfiable if there exists a satisfying assignment of F . To
give an example, the formula in the above example is satisfiable since {x1 (cid:55)→
2, x2 (cid:55)→ 1.05} satisfies it. As another example, the formula (x1 ≥ 2) ∧ (x1 <
1) is unsatisfiable since no assignment satisfies it. Given formulas F and G,
the notation F |= G is used to denote logical consequence or entailment, i.e.,
each assignment that satisfies F also satisfies G. As an illustrative example,
let F = (x1 = 2 ∧ x2 ≥ 1) and G = (2.5x1 + x2 ≥ 5) ∧ (x1 = 1 ∨ x1 = 2).
Then, F |= G. The essence of entailment lies in ensuring the correctness of
the conclusion G based on the given premise F . In the context of computing
explanations, as presented in [8], logical consequence serves as a fundamental
tool for guaranteeing the correctness of predictions made by ANNs. Therefore,
our adaptation of the encoding proposed by Tjeng et al. [17] also incorporates
the principles of entailment for computing explanations.

The relationship between satisfiability and entailment is a fundamental
aspect of logic. It is widely known that, for all formulas F and G, it holds that
F |= G iff F ∧¬G is unsatisfiable. For instance, (x1 = 2∧x2 ≥ 1)∧¬((2.5x1+x2 ≥
5) ∧ (x1 = 1 ∨ x1 = 2)) has no satisfying assignment since an assignment that
satisfies (x1 = 2 ∧ x2 ≥ 1) also satisfies (2.5x1 + x2 ≥ 5) ∧ (x1 = 1 ∨ x1 = 2) and,
therefore, does not satisfy ¬((2.5x1 + x2 ≥ 5) ∧ (x1 = 1 ∨ x1 = 2)). Since our
approach builds upon the concept of logical consequence, we can leverage this
connection in the context of computing explanations for ANNs.

2.2 Mixed Integer Linear Programming

In Mixed Integer Linear Programming (MILP), the objective is to optimize a
linear function subject to linear constraints, where some or all of the variables
are required to be integers [2]. MILP is a crucial technique in our work for
determining the lower and upper bounds of each neuron in the ANNs. For
example, we utilize a minimization problem to determine the lower bound of
neurons within ANNs. This process involves formulating an objective function
that seeks to minimize the lower bound, subject to constraints that reflect the
behaviour of ANNs. To illustrate the structure of a MILP, we provide an example

Comparing Neural Network Encodings for Logic-based Explainability

5

below:

min y1
s.t. 1 ≤ x1 ≤ 3

3x1 + s1 − 2 = y1
0 ≤ y1 ≤ 3x1 − 2
0 ≤ s1 ≤ 3x1 − 2
z1 = 1 → y1 ≤ 0
z1 = 0 → s1 ≤ 0
z1 ∈ {0, 1}

(2)

In the MILP in (2), we want to find values for variables x1, y1, s1, z1
minimizing the value of the objective function y1 among all values that satisfy
the constraints. Variable z1 is binary since z1 ∈ {0, 1} is a constraint in the
MILP, while variables x1, y1, s1 have the real numbers R as their domain. The
constraints in a MILP may appear as linear equations, linear inequalities, and
indicator constraints. Indicator constraints can be seen as logical implications of
the form z = v → (cid:80)n
i=1 wixi ≤ b such that z is a binary variable, v is a constant
0 or 1 [3].

the approach for computing explanations

An important observation is that a MILP problem without an objective
function corresponds to a satisfiability problem, as discussed in Section 2.1.
Given that
relies on logical
consequence, and considering the connection between satisfiability and logical
consequence, we employ a MILP solver
to address explanation tasks.
Additionally, throughout the construction of the MILP model, we utilize
optimization, specifically employing a MILP solver, to determine tight lower
and upper bounds for the neurons of ANNs.

2.3 Classification Problems and Artificial Neural Networks

In machine learning, classification problems are defined over a set of n features
F = {x1, ..., xn} and a set of N classes K = {c1, c2, ..., cN }. In this work, we
consider that each feature xi ∈ F takes its values vi from the domain of real
numbers. Moreover, each feature xi has an upper bound ui and a lower bound
li such that li ≤ xi ≤ ui, and its domain is the closed interval [li, ui]. This is
represented as a set of domain constraints or feature space D = {l1 ≤ x1 ≤
u1, l2 ≤ x2 ≤ u2, ..., ln ≤ xn ≤ un}. For example, a feature for the height of a
person belongs to the real numbers and may have lower and upper bounds of
0.5 and 2.1 meters, respectively. Furthermore, {x1 = v1, x2 = v2, ..., xn = vn}
represents a specific point or instance of the feature space such that each vi is
in the domain of xi.

An ANN is a function that maps elements in the feature space into the set
of classes K. A feedforward ANN is composed of L + 1 layers of neurons. Each
layer l ∈ {0, 1, ..., L} is composed of nl neurons, numbered from 1 to nl. Layer
0 is fictitious and corresponds to the input of the ANN, while the last layer,
K corresponds to its outputs. Layers 1 to L − 1 are typically referred to as

6

L. Carvalho, S. Oliveira and T. Rocha

i,j and bl

hidden layers. Let xl
i ∈ {1, ..., nl}. The inputs to the ANN can be represented as x0
Moreover, we represent the outputs as xL

i be the output of the ith neuron of the lth layer, with
i or simply xi.

i or simply oi.
The values xl
i of the neurons in a given layer l are computed through the
output values xl−1
of the previous layer, with j ∈ {1, ..., nl−1}. Each neuron
j
applies a linear combination of the output of the neurons in the previous layer.
Then, the neuron applies a nonlinear function, also known as an activation
function. The output of the linear part is represented as (cid:80)nl−1
j + bl
i
where wl
i denote the weights and bias, respectively, serving as parameters
of the ith neuron of layer l. In this work, we consider only feedforward ANNs
with the Rectified Linear Unit (ReLU) as activation function because it can be
represented by linear constraints due to its piecewise-linear nature. This function
is a widely used activation whose output is the maximum between its input value
i,jxl−1
and zero. Then, xl
For classification tasks, the last layer L is composed of nL = N neurons, one
for each class. Moreover, it is common to normalize the output layer using a
Softmax layer. Consequently, these values represent the probabilities associated
with each class. The class with the highest probability is chosen as the predicted
class. However, we do not need to consider this normalization transformation
as it does not change the maximum value of the last layer. Thus, the predicted
class is ci ∈ K such that i = arg maxj∈{1,...,N } xL
j .

i) is the output of the ReLU.

i = ReLU((cid:80)nl−1

j + bl

i,jxl−1

j=1 wl

j=1 wl

3 Explanations for ANNs with Logical Implications

Ignatiev et al. [8] proposed an algorithm that computes minimal explanations for
ANNs, yielding a subset of the input features sufficient for the prediction. This
approach is based on logic with guarantees on the correctness and minimality of
explanations. A flowchart for computing explanations using such an algorithm
is shown in Figure 1.

Fig. 1. Flowchart for calculating explanations.

First, the ANN and the feature space {l1 ≤ x1 ≤ u1, l2 ≤ x2 ≤ u2, ..., ln ≤
xn ≤ un} are encoded as a formula F , an instance {x1 = v1, x2 = v2, ..., xn =

Comparing Neural Network Encodings for Logic-based Explainability

7

vn} of the feature space is encoded as a conjunction in a formula C, and the
associated prediction by the ANN is encoded as a formula E. Then, it holds
that C ∧ F |= E. The minimal explanation Cm of C is calculated removing
feature by feature from C. For example, given a feature xi with value v in C,
if C \ {xi = v} ∧ F |= E, feature xi may be considered as irrelevant in the
explanation and is removed from C. Otherwise, if C \ {xi = v} ∧ F ̸|= E, then xi
is kept in C since the same class cannot be guaranteed. This C\{xi = v} notation
represents the removal of xi = v from formula C. This process is described in
Algorithm 1 and is performed for all features. Then, Cm is the result at the
end of this procedure. This means that for the values of the features in Cm,
the ANN makes the same classification, whatever the values of the remaining
features. Since to check entailments C ∧ F |= E is equivalent to test whether
C ∧ F ∧ ¬E is unsatisfiable and F , C and ¬E are enconded as linear constraints
and indicator constraints, such a entailment can be addressed by a MILP solver.

Algorithm 1 Computing a minimal explanation
Input: ANN and domain constraints F , input data C, prediction E
Output: minimal explanation Cm
1: for xi = v in C do
2:
3:
4: Cm ← C
5: return Cm

if C \ {xi = v} ∧ F |= E then

C ← C\{xi = v}

The encoding of ANNs used in [8] and originally proposed by Fischetti and Jo
[5] uses implications to represent the behavior of the ReLU activation function.
We encode an ANN with L + 1 layers as in Equations (3)-(5). In the following,
we explain the notation. The encoding uses variables xl
i and on with the same
i and zl
meaning as in the notation for ANNs. Auxiliary variables sl
i control the
behaviour of ReLU activations. Variable zl
i is binary and if zl
i is equal to 1, the
ReLU output xl
i is 0 and −sl
i is equal to the linear part. Otherwise, the output
i is equal to the linear part and sl
xl
s,i is the upper
bound of variable sl
x,i is the upper bound of variable xl
i.
Each variable x0
i has also lower and upper bounds li, ui, respectively, defined by

i is equal to 0. The constant ubl

i, and the constant ubl

8

L. Carvalho, S. Oliveira and T. Rocha

the domain of the features.
nl−1
(cid:88)

wl

i,jxl−1

i + bl

i = xl

i − sl
i

n ≤ 0
n ≤ 0

j=1
i = 1 → xl
zl
i = 0 → sl
zl
zl
i ∈ {0, 1}
0 ≤ xl
0 ≤ sl

i ≤ ubl
i ≤ ubl
s,i

x,i






l = 1, ..., L − 1, i = 1, ..., nl

(3)

i = 1, ..., n0

li ≤ xi ≤ ui,
nL−1
(cid:88)

oi =

wL

i,jxL−1
i

+ bL
i ,

i = 1, ..., nL

(4)

(5)

j=1

The constraints in (3)-(5) represent the formula F . The bounds ubl

x,i are
defined by isolating variable xl
i from other constraints in subsequent layers. Then,
xl
i is maximized to find its upper bound. A similar process is applied to find the
bounds ubl
i. This optimization is possible due to the bounds of
the features. Furthermore, these bounds can assist the solver in accelerating the
computation of explanations. Therefore, the time required for this process must
be considered when building F .

s,i for variables sl

To check the unsatisfiability of the expression C ∧ F ∧ ¬E, we still need to
take into account the formula ¬E, referring to the prediction of the ANN. Given
an input C predicted as class ci by the ANN, formula E must be equivalent to
(cid:86)N
j=1,j̸=i oi > oj. This formula asserts that the maximum value of the last layer
is in output oi. Therefore, ¬E must ensure that (cid:87)N
j=1,j̸=i oi ≤ oj. Since MILP
solvers can not directly represent disjunctions, we use implications (6) and a
linear constraint (7) over binary variables to define ¬E.

qj = 1 → oi ≤ oj,

j ∈ {1, ..., N } \ {i}

N
(cid:88)

qj ≥ 1

j=1,j̸=i
qj ∈ {0, 1},

j ∈ {1, ..., N } \ {i}

(6)

(7)

(8)

If an assignment A satisfies (cid:87)N

j=1,j̸=i oi ≤ oj, then oi ≤ oj is true under
A for some j. Therefore, the assignment A ∪ {qj (cid:55)→ 1} satisfies the Equations
(6)-(8). Conversely, if an assignment A satisfies Equations (6)-(8), it clearly also
satisfies (cid:87)N
j=1,j̸=i oi ≤ oj. We conclude this section with a proposition regarding
the number of variables and constraints of the encoding discussed above.

Proposition 1. Let C be an instance predicted as a class c ∈ K by an ANN, the
formula C ∧F ∧¬E has n0 +nL +2 (cid:80)nL−1
l=1 nl
binary variables. Also, the formula has n0 + 2nL + 5 (cid:80)nL−1

l=1 nl real variables and nL −1+(cid:80)nL−1

constraints.

l=1

Comparing Neural Network Encodings for Logic-based Explainability

9

4 Explanations for ANNs without Implications

In this section, we present an adaptation of the encoding proposed by
Tjeng et al. [17] for logic-based explainability. In such a work, the authors
originally used the encoding to find adversarial examples without using logical
implications. Even more importantly, such an encoding uses fewer variables and
constraints compared to [5]. Then, we expect that our adaptation can lead to
a better execution time for both building the logical constraints and computing
explanations. Adapting the encoding in [17] to the context of computing
explanations requires incorporating additional constraints that were not part
of the original work. These new constraints represent the class predicted by the
ANN as a formula E, as seen in Section 3. However, to maintain the concept of
the original encoding without implications, we define these additional constraints
without implications.

i are, respectively, the lower and upper bounds of (cid:80)nl−1

In the following, we apply the same algorithm from Figure 1, but replacing
the encoding of F as in [5] with the one in [17]. We encode an ANN with L + 1
layers as in Equations (9), (4) and (5). The variables xl
i and oi have the same
meaning as in Equations (3)-(5). Furthermore, auxiliary variables sl
i are not
required, as observed in the encoding by Fischetti and Jo [5]. Constants lbl
i and
ubl
i. Again,
we find such bounds via a MILP solver. The behavior of ReLU is modeled using
these bounds and binary variables zl
i. If zl
i is
i,jxl−1
0. Otherwise, xl
i. The bounds lbli and ubli are
j=1 wl
necessary to maintain the integrity of the set of constraints for the entire feature
space. Regardless of the value of zli, the bounds ensure that the constraints
remain valid for the entire feature space.

i is equal to 0, the ReLU output xl
j + bl

i is equal to (cid:80)nl−1

j + bl

i,jxl−1

j=1 wl

wl

i,jxl−1

j + bl

i − lbl

i(1 − zl
i)

wl

i,jxl−1

j + bl
i

nl−1
(cid:88)

j=1
nl−1
(cid:88)

xl
i ≤

xl
i ≥

j=1
i ≤ ubl
xl
izl
i
zl
i ∈ {0, 1}
xl
i ≥ 0






l = 1, ..., L − 1, i = 1, ..., nl

(9)

In our proposal for computing explanations, constraints in Equations (9), (4)
and (5) represent the formula F . As in Section 3, an instance is a conjunction
C, and the associated prediction by the ANN is a formula E. Given an input
C predicted as class ci by the ANN, again formula ¬E must ensure that
(cid:87)N
j=1,j̸=i oi ≤ oj. Therefore, we must add new constraints to represent ¬E.
Maintaining the concept of the original encoding in [17] without implications,
we define these additional constraints accordingly. We employ binary variables
qj and the upper and lower bounds ubj and lbj of variables oj. As for lbl
i and ubl
i,
we find the bounds ubj and lbj through a MILP solver. We recall such elements

10

L. Carvalho, S. Oliveira and T. Rocha

are not originally present in [17]. However, they are necessary for the context
of computing explanations for ANNs. In Equations (10)-(12) we represent our
proposal for encoding formula ¬E, where the prediction associated with an input
C is class ci.

oi − oj ≤ (ubi − lbj)(1 − qj),

j ∈ {1, ..., N } \ {i}

N
(cid:88)

qj ≥ 1

j=1,j̸=i
qj ∈ {0, 1},

j ∈ {1, ..., N } \ {i}

(10)

(11)

(12)

In what follows, we prove that Equations (10)-(12) correctly ensure that

(cid:87)N

j=1,j̸=i oi ≤ oj.

Proposition 2. Let ¬E be defined as in Equations (10)-(12). Let i ∈ {1, ..., N }
be fixed and ubi be such that oi ≤ ubi. Let lbj be such that lbj ≤ oj, for j ∈
{1, ..., N } \ {i}. Therefore,

¬E is satisfiable iff

N
(cid:95)

j=1,j̸=i

oi ≤ oj is satisfiable.

j=1,j̸=i oi ≤ oj, then oi ≤ o′

Proof. If an assignment A satisfies (cid:87)N
j is true under
̸= i. Let A′ = A ∪ {qj (cid:55)→ v | v = 1 if j = j′, else v = 0, for j ∈
A for j′
{1, ..., N }\{i}} be an assignment. Then, A′ imposes that oi ≤ o′
j in Equation (10)
for j = j′, which in clearly true under this assignment. For j ̸= j′, it follows that
oi − oj ≤ (ubi − lbj) must hold, which is also true under A′ since lbj ≤ oj and
oi ≤ ubi.

Conversely, if an assignment A satisfies Equations (10)-(12), it satisfies some
j, for j′ ̸= i by Equation (11). Moreover, A satisfies oi ≤ oj′ by Equation (10),
q′
for j = j′. Therefore, A also satisfies (cid:87)N

j=1,j̸=i oi ≤ oj.

Finally, we give a proposition on the number of variables and constraints in

our adaptation of the encoding in [17].

Proposition 3. Let C be an instance predicted as a class c ∈ K by an ANN,
then formula C ∧ F ∧ ¬E has n0 + nL + (cid:80)nL−1
l=1 nl real variables and nL − 1 +
(cid:80)nL−1
l=1 nl

l=1 nl binary variables. Moreover, the formula has n0 + 2nL + 4 (cid:80)nL−1

constraints.

Therefore, this encoding has (cid:80)nL−1

l=1 nl fewer variables than the one presented
in Section 3. Additionally, this encoding has (cid:80)nL−1
fewer constraints.
l=1 nl
Consequently, one would expect a reduction in running time for both building
the logical constraints and computing explanations.

Comparing Neural Network Encodings for Logic-based Explainability

11

5 Experiments

In this section, we detail the experiments conducted to compare our proposal
against the encoding presented in [5]. Our evaluation consists of two main
experiments. In the first one, we compare the two encodings using 12 datasets.
In the second one, we conduct a detailed comparison using a single dataset. We
vary the architecture of the trained ANNs to explore the effect of the number of
layers and neurons. We evaluate the performance of each encoding in terms of
time for building logical constraints and time for computing explanations. We
explained all instances in a given dataset and calculated the average time and
standard deviation to compare times for computing explanations. In the other
case, given a trained ANN on a dataset, we built the logical constraints 10 times
and calculated the average time and standard deviation.

trained ANNs and hyperparameters. After

Next, we present the experimental setup, describing technologies, datasets,
the
the
results providing a comparative analysis of the running times for building
logical constraints and computing explanations. Finally, we highlight specific
improvements observed in our proposal.

that, we discuss

5.1 Experimental Setup

We used Python to implement the approaches and to run the experiments.
TensorFlow was used to manipulate ANNs, including the training and testing
steps. CPLEX was used as the MILP solver and accessed by the DOcplex library.
We used 12 datasets from the UCI Machine Learning Repository2 and Penn
Machine Learning Benchmarks3, each ranging from 9 to 32 integer, continuous,
categorical or binary features. The number of instances in the selected datasets
ranges from 156 to 691. The types of classification problems related to
these datasets are binary and multi-class classification. The preprocessing
performed on the datasets included one-hot encoding of the categorical data and
normalization of the continuous features to the range [0, 1]. This normalization
was not applied to the integer features to avoid transforming their space into
continuous, which could compromise formal guarantees on the correctness of the
algorithm. As far as we know, such a methodology was not considered in earlier
works.

The ANNs training was accomplished using a batch size of 4 and a maximum
of 100 epochs, applying early stopping regularization with 10 epochs based on
validation loss. The optimization algorithm used was Adam and the learning
rate was 0.001. The datasets split was 80% for training and 20% for validation.
The ANN architectures were limited to 2 layers to reduce the total running
time, because many solver calls were performed in the experiments due to the
large number of instances. Each solver call deals with an NP-complete problem,
therefore, impacting the experiments running time.

2 https://archive.ics.uci.edu/ml/
3 https://github.com/EpistasisLab/penn-ml-benchmarks/

12

L. Carvalho, S. Oliveira and T. Rocha

Datasets

breast-cancer (9)
glass (9)
glass2 (9)
cleve (13)
cleveland (13)
heart-statlog (13)
australian (14)
voting (16)
hepatitis (19)
spect (22)
auto (25)
backache (32)

Fischetti and Jo [5]

Our Proposal

Exp (s)
0.39 ±0.24
0.25 ±0.09
0.39 ±0.29
0.51 ±0.23
0.69 ±0.8
0.41 ±0.18
0.76 ±0.43
1.02 ±0.52
1.17 ±1.27
2.64 ±1.56
0.79 ±0.3
11.44 ±10.3

Build (s)
2.98 ±0.05
4.25 ±0.18
3.45 ±0.1
4.26 ±0.14
5.28 ±0.11
3.76 ±0.05
3.2 ±0.08
3.62 ±0.07
5.66 ±0.14
4.81 ±0.13
6.86 ±0.25
5.04 ±0.16

Exp (s)
0.39 ±0.21
0.26 ±0.1
0.47 ±0.34
0.54 ±0.25
0.82 ±1.03
0.41 ±0.18
0.84 ±0.42
1 ±0.47
1.1 ±1.13
3.21 ±1.82
0.83 ±0.32
11.39 ±9.74

Build (s)
2.67 ±0.17
3.86 ±0.09
3.02 ±0.02
3.55 ±0.01
4.57 ±0.17
3.06 ±0.02
3.14 ±0.72
3.01 ±0.02
5.28 ±0.07
5.74 ±0.15
6.62 ±0.14
4.78 ±0.14

Table 1. Comparison of both encodings.

The first experiment compared the two encodings presented on the 12
datasets. The architecture of the trained ANNs is two hidden layers with 20
neurons each. For each dataset and the associated ANN, the explanation of each
instance was obtained using the Algorithm 1 and both presented encodings. The
second experiment performed the comparison of the two encodings presented
using the voting dataset of the first experiment. This experiment was conducted
in two cases. In the first case, the trained ANNs consist of one hidden layer with
the number of neurons ranging from 10 to 100. In the second case, the ANNs
consist of two hidden layers such that both layers contains the same number of
neurons, ranging from 10 to 40. In both cases, the number of neurons in the
layers increases in increments of 5. Again, the explanations of each instance was
obtained using Algorithm 1 and both presented encodings. The objective of this
experiment is to verify the influence of the number of layers and the number of
neurons in both encodings.

5.2 Results

The results of the first experiment are shown in Table 1. For each dataset, its
number of features is indicated in parentheses. The column Exp (s) refers to the
average running time for computing explanations in seconds, and the standard
deviation is also presented. The column Build (s) refers to the average running
time, in seconds, for building the logical constraints of the trained ANN. The
running time for finding the bounds of variables is included in the time for
building the encodings.

Despite the encoding by Fischetti and Jo [5] achieved a better average running
time for computing explanations in 7 out of 12 cases, both encodings generally
perform similarly when considering the variability of the time. For instance, in
the spect dataset, the average execution time of the encoding by Fischetti and
Jo [5] (2.64 seconds) falls within the range of the average minus the standard
deviation of our approach (3.21 − 1.81). This pattern is observed across several

Comparing Neural Network Encodings for Logic-based Explainability

13

Fig. 2. Comparison of average running time for computing explanations, using ANNs
with one hidden layer and the voting dataset.

datasets. In summary, the results indicate that both encodings generally perform
similarly in terms of running time for computing explanations, with minor
variations across different datasets.

With respect to the average running time for building the logical constraints,
our adaptation generally outperformed the encoding by Fischetti and Jo [5].
However, there were exceptions noted, such as in the spect dataset. Overall, our
adaptation achieved an improvement of up to 18% compared to the the other one,
as seen in the heart-statlog dataset. In summary, the results indicate that our
adaptation is consistently more efficient than the other approach for building
logical constraints. The variability in the results, as shown by the standard
deviations, further supports this conclusion. For instance, the average running
time of our proposal is less than the average minus the standard deviation of the
other approach in 9 out of 12 datasets. These cases are highlighted in bold in
Table 1. It is important to note that, in the spect dataset, the average running
time of the encoding proposed by Fischetti and Jo [5] is less than the average
minus the standard deviation of our adaptation. This indicates that, while our
proposal appears generally more efficient, specific dataset characteristics can
influence which encoding is more advantageous. Our adaptation yielded notably
superior results not only in the average time for building logical constraints
but also in the overall time, which includes both computing explanations and
constructing logical constraints. For instance, it achieved an improvement of up
to 16% compared to the other approach, as seen in the heart-statlog dataset.

The results of the second experiment are shown in Figures 2 and 3. The x-axis
shows the number of neurons in each hidden layer in both Figures. While the
y-axis shows the average running time for computing explanations. Furthermore,

14

L. Carvalho, S. Oliveira and T. Rocha

Fig. 3. Comparison of average running time for computing explanations, using ANNs
with two hidden layers and the voting dataset.

the standard deviation is indicated by the shaded region in both Figures. Figure
2 refers to ANNs with one hidden layer, while Figure 3 refers to ANNs with two
hidden layers.

Figure 2 suggests that our proposal, for ANNs with one hidden layer,
achieved a superior average running time for computing explanations in
the voting dataset. This encoding outperforms the other approach with
percentage improvements ranging from approximately 7.69% to 40.82%. The
most significant improvements are observed with higher neuron counts. For
instance, with 85 neurons per layer, our proposal shows an improvement
of around 40.82%, and with 95 neurons, the improvement is about 31.65%.
Moreover, our proposal generally exhibits similar or lower standard deviations,
indicating more consistent performance across different number of neurons.

On the other hand, Figure 3 depicts comparable results between both
encodings for ANNs with two hidden layers. Moreover, similar standard
deviations were achieved for both encodings, indicating comparable levels of
consistency in performance. The variability increases significantly with more
complex networks. For example, with 35 and 40 neurons per layer, both
encodings exhibit significant variability. Furthermore, with 30 neurons per layer,
our adaptation shows higher variability compared to the other approach.

6 Conclusions and future work

Explanations for the outputs of ANNs are fundamental in many scenarios,
due to critical systems, data protection laws, adversarial examples, among
others. Therefore, several heuristic methods have been developed to provide
explanations for the decisions made by ANNs. However, these approaches
lack guarantees of correctness and may also produce redundant explanations.
Logic-based approaches address these issues but often suffer from scalability
problems.

In this work, we compare two logical encodings of ANNs: one has been used
in the literature to provide explanations [5, 8], and another [17] that we have
adapted for our context of explainability. Our experiments indicate that both
encodings have similar running times for computing explanations, even as the

Comparing Neural Network Encodings for Logic-based Explainability

15

number of neurons and layers increases. Furthermore, our experimental results
suggest that our adaptation is generally more efficient for ANNs with one hidden
layer, while the performance advantage diminishes for ANNs with two hidden
layers. However, our proposal achieved a better running time for building the
encoding for ANNs with two layers, showing an improvement of up to 18%. This
can help to decrease the scalability issue for building the logical constraints given
an ANN. Furthermore, this encoding obtained better results also in the overall
time, i.e., the time for computing explanations plus the time for building the
logical constraints, showing an improvement of up to 16%.

In the experiments of this work, we considered all instances of the datasets
used, which considerably increased the experiments running time. As future
work, we can change the design of experiments, using only a subset of the
datasets, to allow the use of larger ANNs. More experiments are necessary,
especially with additional layers and neurons, to further validate our findings and
understand the performance of these encodings. Furthermore, others encodings
[9, 10] can be evaluated for computing logic-based minimal explanations for
ANNs. Moreover, in order to improve the scalability of computing logic-based
explanations, the ANNs can be simplified, before or during building their
encodings, via pruning or slicing as proposed by [12]. This results in equivalent
ANNs with smaller sizes.

References

[1] Audemard, G., Lagniez, J.M., Marquis, P., Szczepanski, N.: Computing

abductive explanations for boosted trees. In: 26th AISTATS (2023)

[2] Bénichou, M., Gauthier, J.M., Girodet, P., Hentges, G., Ribière,
linear programming.

G., Vincent, O.: Experiments in mixed-integer
Mathematical Programming 1, 76–94 (1971)

[3] Bonami, P., Lodi, A., Tramontani, A., Wiese, S.: On mathematical
programming with indicator constraints. Mathematical Programming
151(1), 191–223 (2015)

[4] Choi, A., Shih, A., Goyanka, A., Darwiche, A.: On symbolically encoding

the behavior of random forests. In: 3rd FoMLAS (2020)

[5] Fischetti, M., Jo, J.: Deep neural networks and mixed integer linear

optimization. Constraints 23, 296–309 (2018)

[6] Goodfellow,

I.J., Shlens, J., Szegedy, C.: Explaining and harnessing

adversarial examples. In: 3rd ICLR (2015)

[7] Gorji, N., Rubin, S.: Sufficient reasons for classifier decisions in the presence

of domain constraints. In: 36th AAAI (2022)

[8] Ignatiev, A., Narodytska, N., Marques-Silva, J.: Abduction-based

explanations for machine learning models. In: 33rd AAAI (2019)

[9] Katz, G., Barrett, C., Dill, D.L., Julian, K., Kochenderfer, M.J.: Reluplex:
An efficient SMT solver for verifying deep neural networks. In: 29th CAV
(2017)

16

L. Carvalho, S. Oliveira and T. Rocha

[10] Katz, G., Huang, D.A., Ibeling, D., Julian, K., Lazarus, C., Lim, R., Shah,
P., Thakoor, S., Wu, H., Zeljić, A., Dill, D.L., Kochenderfer, M.J., Barrett,
C.: The marabou framework for verification and analysis of deep neural
networks. In: 31st CAV (2019)

[11] Kroening, D., Strichman, O.: Decision procedures. Springer (2016)
[12] Lahav, O., Katz, G.: Pruning and slicing neural networks using formal

verification. In: 21st FMCAD (2021)

[13] Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., Alsaadi, F.E.: A survey of
deep neural network architectures and their applications. Neurocomputing
234, 11–26 (2017)

[14] Ribeiro, M.T., Singh, S., Guestrin, C.:

“Why should I trust you?”:

Explaining the predictions of any classifier. In: 22nd KDD (2016)

[15] Ribeiro, M.T., Singh, S., Guestrin, C.: Anchors: High-precision

model-agnostic explanations. In: 32th AAAI (2018)

[16] Shih, A., Choi, A., Darwiche, A.: A symbolic approach to explaining

bayesian network classifiers. In: 27th IJCAI (2018)

[17] Tjeng, V., Xiao, K.Y., Tedrake, R.: Evaluating robustness of neural

networks with mixed integer programming. In: 7th ICLR (2019)

[18] Wu, M., Wu, H., Barrett, C.: Verix: Towards verified explainability of deep

neural networks. In: 37th NeurIPS (2023)

