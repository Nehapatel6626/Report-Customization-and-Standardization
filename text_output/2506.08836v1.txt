5
2
0
2

n
u
J

0
1

]
L
C
.
s
c
[

1
v
6
3
8
8
0
.
6
0
5
2
:
v
i
X
r
a

Advancing STT for Low-Resource Real-World
Speech

Flavio D’Intino1[0009−0007−2245−7799] and Hans-Peter
Hutter1[0000−0002−1709−546X]

Institute of Computer Science, Zurich University of Applied Sciences, Winterthur,
Switzerland
{dint,huhp}@zhaw.ch

Abstract. Swiss German is a low-resource language represented by di-
verse dialects that differ significantly from Standard German and from
each other, lacking a standardized written form. As a result, transcrib-
ing Swiss German involves translating into Standard German. Existing
datasets have been collected in controlled environments, yielding effec-
tive speech-to-text (STT) models, but these models struggle with spon-
taneous conversational speech.
This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from
39 Swiss German radio and TV stations. It captures spontaneous speech
across all major Swiss dialects recorded in various realistic environments
and overcomes the limitation of prior sentence-level corpora.
We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance
metrics. Improvements in word error rate (WER) ranged from 19% to
33%, while BLEU scores increased between 8% and 40%. The best fine-
tuned model, large-v3, achieved a WER of 17.1% and a BLEU score of
74.8. This advancement is crucial for developing effective and robust STT
systems for Swiss German and other low-resource languages in real-world
contexts.

Keywords: ASR · Speech-to-Text · Low-Resource Languages

1

Introduction

Swiss German, a collection of Alemannic dialects spoken by about 3.9 million
people in Switzerland,1 is known for its diversity with hundreds of local di-
alects varying significantly across regions.2 Swiss German significantly differs
from Standard German in phonetics, vocabulary, morphology, and syntax, mak-
ing it difficult for German listeners to understand without training.

Unlike Standard German, Swiss German lacks a standardized written form
and is typically only used in informal written contexts, such as text messages.

1 Bundesamt für Statistik: Hauptsprachen seit 1910, accessed 29.07.2024
2 Federal Departement of Foreign Affairs - Languages and dialects, accessed 09.10.2024

 
 
 
 
 
 
2

F. D’Intino, H.-P. Hutter

Therefore, transcribing Swiss German is conventionally done in Standard Ger-
man, which is considered a translation task rather than a mere transcription.
This translation comes with the corresponding difficulties, e.g., due to spelling
ambiguities or only partly matching vocabularies [16]. Phonetic, morphological,
and syntactic variations across Swiss German dialects and even within the same
dialect further complicate the transcription task.

Section 2 provides an overview of the most common speech corpora for Swiss
German and the SOTA STT models trained with these datasets. In Section 3,
we discuss the details of these datasets and highlight their limitations, as well as
the limitations of the SOTA models in real-world transcription tasks. Section 4
presents the specifics of the new SRB-300 speech corpus and the methodology
used for its collection. In Section 5, we explain how we fine-tuned various Ope-
nAI Whisper models using SRB-300 for practical applications in low-resource
languages, specifically Swiss German. Finally, Section 6 compares the perfor-
mance of the different fine-tuned Whisper models against other SOTA models
on the new SRB-300 corpus.

2 Related Work

Several efforts have been made to collect Swiss German speech corpora for train-
ing and optimizing STT models [6,20,21,22]. These datasets were gathered in rel-
atively controlled settings, such as partially read speech, indoor environments,
single speakers, and minimal background noise. Using these datasets, various
deep learning-based STT models for Swiss German have been trained, includ-
ing Conformer [10] and the foundation model XLS-R [1]. These models have
performed well on their respective test sets [20,22], as summarized in Section 3.
Similarly, OpenAI’s Whisper models [24] have also been evaluated for Swiss
German [7,26] using these datasets. The Whisper models showed surprisingly
good zero-shot performance, although they were slightly less effective than mod-
els specifically trained on the Swiss German datasets. Recently, Whisper models
have been fine-tuned on these datasets, as described in Section 3, employing
different sample concatenation strategies [28]. While the fine-tuned models out-
performed the previously best models on the available datasets, they still face
challenges with more realistic speech data.

3 Existing Datasets for Swiss German

Several publicly available speech corpora for Swiss German have been collected
in recent years.

SwissDial The SwissDial parallel corpus [6] consists of 26 hours of single-
sentence samples. Each sentence is spoken by 8 different speakers, each from
a different region of Switzerland, resulting in approximately 3 hours of record-
ings for each dialect. The origin of the utterances is written texts in Standard

Advancing STT for Low-Resource Real-World Speech

3

German, including news articles, weather reports, and Wikipedia entries. The
duration of the samples varies from 0.5 s to 13.5 s, with an average duration of
3.3 s.

SPC The Swiss Parliaments Corpus (SPC) [22] consists of 299 hours of tran-
scribed recordings of the Bernese parliament "Grosser Rat". The data is divided
into a training set of 293 hours and a test set of 6 hours. Since the parliament is
located in Bern, most speakers use a Bernese dialect. The annotations are not
entirely accurate transcriptions. The duration of the samples varies from 0.1 s to
108 s, with an average duration of 7.1 s.

SDS-200 The SDS-200 dataset [21] comprises 200 hours of spoken audio recorded
by approximately 4000 speakers. However, the number of recordings varies signif-
icantly among speakers, following a long-tail distribution. The test set contains
5 hours of audio. An online service was utilized in which speakers were presented
with a Standard German sentence to be spoken in their respective Swiss German
dialects. To ensure quality, the recordings were validated by other participants.
The sentences were selected from Swiss newspapers and the Standard German
dataset of the Common Voice corpus.3 The sample durations range from 2.0 s to
11.2 s, with an average duration of 4.8 s.

STT4SG-350 The STT4SG-350 [20] consists of 343 hours of speech recorded
by 316 different speakers, with the test set containing 34 hours. This dataset was
collected in the same manner as the SDS-200 dataset. The written sentences are
derived from news articles and recordings of the Swiss parliament. The sample
durations range from 2.0 s to 15.4 s, with an average duration of 5.0 s.

3.1 Limitations of these Datasets

The aforementioned corpora are particularly valuable for researching STT mod-
els and benchmarking purposes. However, models trained on these datasets face
challenges when transcribing realistic, spontaneously spoken Swiss German, par-
ticularly in everyday conversational situations. These datasets have one or more
of the following limitations:

– short samples (a few seconds only) with a single utterance
– controlled recordings in quiet environments
– few spontaneous speech: utterances either translated from Standard Ger-
man sentences or read from a manuscript, therefore hardly any spontaneous
speech characteristics (disfluencies, hesitations, slip of the tongue, repeti-
tions, etc.)

– only one single speaker per sample

3 https://commonvoice.mozilla.org

4

F. D’Intino, H.-P. Hutter

The SOTA STT models trained with these datasets perform exceptionally
well on the corresponding test sets (see Table 1). However, their performance
dramatically drops on more realistic, out-of-distribution datasets, such as the
new SRB-300 corpus (see Table 4). The short samples in these datasets pose

Table 1. Reported results of the fine-tuned SOTA models on the test sets of the
most common scientific corpora for Swiss German. For reference, we include the zero-
shot results of the pre-trained XLS-R 1B model on the new SRB-300 corpus, which is
introduced in this paper.

WER BLEU fine-tuned Model
Corpus
23.7
SPC
SDS-200
18.2
STT4SG-350 14.0
44.4
SRB-300

60.7 XLS-R 1B, [25]
69.6 XLS-R 1B, [20]
74.7 XLS-R 1B, [20]
37.5 XLS-R 1B, this work (ref. Table 4)

significant challenges for training models like Whisper, which require an input
length of 30 seconds. This necessitates substantial zero-padding for each train-
ing sample, leading to difficulties when the trained model is applied to longer
audio sequences. Timmel et al. recently tackled this issue by combining several
independent samples from the available datasets into 30-second samples using
various strategies. Their resulting model surpassed the previous SOTA models
across all datasets, yet it remained inferior to the original Whisper model when
tested on more realistic, longer audio data [28].

These findings highlight the necessity of addressing the aforementioned dataset
limitations to enhance the performance of models on conversational Swiss Ger-
man speech in various real-world contexts. To this end, we have compiled a new
corpus specifically addressing these limitations.

4 New Real-World Speech Corpus for Swiss German

We introduce the new SRB-300 (300 h Swiss Regional Broadcasts) dataset, an
annotated speech corpus that addresses the limitations of previous Swiss German
datasets. The SRB-300 dataset includes 303 hours of audio recordings from 39
regional Swiss German radio and TV broadcast stations across German-speaking
Switzerland. It captures realistic and spontaneous speech in a variety of contexts,
such as news reports, interviews, discussions, and live events from both indoor
and outdoor settings. This dataset offers a representative selection of all major
Swiss dialects and is designed for training with long audio samples. The following
sections detail the dataset and describe the collection process.

4.1 Data Collection and Pre-Processing

The speech data was recorded from radio and TV broadcasts. To extract 30 s
speech samples for the datasets, the following steps were performed.

Advancing STT for Low-Resource Real-World Speech

5

1. Extraction: extract speech segments from the original audio
2. Pre-transcription: automatically pre-transcribe the extracted speech audio
3. Correction: manually correct the pre-transcription according to given rules
4. Sample generation: chunk utterances into up to 30 s long samples

Extraction Each available MP4 recording from 29 radio and 10 TV stations
contained approximately 8-hour excerpts of a day’s radio and TV broadcast,
including non-speech sections, resulting in a total of about 1790 hours of audio.
In the first step, irrelevant parts, such as music, jingles, commercials, and re-
peated programs, were identified and removed from the recordings. Professionals
then manually labeled the remaining speech data with contextual metadata, in-
cluding details like speaker gender and type of broadcast. One-third of the data
comes from TV broadcasts, while two-thirds comes from radio (ref. Appendix A,
Table 6).

Pre-Transcription The approximately 300 hours of speech from the extrac-
tion step were automatically pre-transcribed using OpenAI’s Whisper large-v34
model. The pre-transcribed utterances were then joined into 28-second speech
audio chunks using the following process:

– Successive speech segments were merged into whole utterances (usually a
sentence) by merging their timestamps and transcripts. For example, the
two pre-transcribed segments:

(307.62, 300.7) Und auch am Mittag und Nachmittag ist

(300.7, 312.3) häufig sogar wolkenlos über dem Jura.

der Himmel meistens blau,

were merged into the following utterance

(307.62, 312.3) Und auch am Mittag und Nachmittag ist

der Himmel meistens blau, häufig sogar
wolkenlos über dem Jura.

– Timestamps were rounded to whole seconds because the transcription tool

used could only handle integer timestamps.

– The German character ‘ß‘ was replaced with ’ss’, as it does not exist in Swiss

German.

– Successive utterances were merged to segments of up to 28 s.5

The last step involved creating XML files that included the combined transcrip-
tions. These files, along with the audio files, served as the input data for the
transcription tool used in the next step.

4 https://huggingface.co/openai/whisper-large-v3
5 Finally, we aimed for a maximum sample length of 30 s. Therefore, we left some
margins so that the segment boundaries could be adjusted during the later manual
correction step.

6

F. D’Intino, H.-P. Hutter

Manual Correction The pre-transcriptions from the previous phase were man-
ually reviewed and adjusted according to predefined rules (see Appendix D).
Along with the transcription text, the start and end timestamps of the segments
were revised to ensure that the entire audio content of the transcription was
included as accurately as possible.

By providing automated pre-transcriptions, the transcription time required
for lay transcribers was reduced by a factor of 2 to 3. On average, the time to
check and correct the transcription was 3 to 4 times the audio duration.

Sample Generation The samples were extended by 0.2 seconds at both the
beginning and the end to ensure that no utterances were inadvertently cut off due
to inaccuracies in the rounded timestamps. Each sample was assigned a random
identifier and automatically supplemented with metadata, including details such
as the broadcast station, speaker gender, and its position (index) within the
entire broadcast. A detailed description of each available attribute is provided
in Table 2. This data was obtained from the original metadata accompanying
the raw audio data.

The sample generation process resulted in the sample length distribution
depicted in Figure 1. Thus, the samples are considerably longer than in the cor-
pora in section 3. It is important to note that samples with lengths significantly
shorter than 30 seconds typically occur at the end of the original audio file or
cannot be merged with adjacent segments, as doing so would lead to sample
lengths exceeding 30 seconds.

Figure 2 shows the geographic distribution of the broadcasting stations, and
Figure 3 the length distribution of the audio samples in the SRB-300 dataset.
To that end, the broadcasting stations were categorized into 10 dialect regions.
The diameters of the circles in Figure 2 represent the total duration of samples
collected from broadcast stations within each region. We have ensured that the
SRB-300 corpus is weighted to reflect the size of their speaker populations.6 Since
regional broadcasting stations generally feature local presenters and cater to local
listeners, we can assume that the dialects of the speakers closely align with the
locations of these stations. Therefore, we anticipate that the most prominent
Swiss dialects are well represented in the dataset. The green background in
Figure 2 indicates the approximate border of the Swiss German language in
Switzerland.7

The dataset was not artificially gender-balanced, but based on the available
metadata, we estimate a male-to-female speaker ratio of approximately 2:1 (ref.
Table 7 in Appendix A).

The broadcasts cover a wide variety of formats, including entertainment,
information, news, traffic updates, weather reports, and moderation in both

6 The majority of the Swiss population lives in the "Mittelland," located north of the

Alps between Geneva and Lake Constance.

7 The Swiss German language border is approximated according to Die 4 Sprachgebi-

ete der Schweiz nach Gemeinden, 2020, accessed 18.09.2024

Advancing STT for Low-Resource Real-World Speech

7

Fig. 1. Distribution of sample duration over all datasets in SRB-300. The durations
range from 3.4 s to 30.0 s, with an average of 25.4 s. Each of the three datasets (ref.
Section 4.2) has a similar distribution.

Table 2. Metadata attributes of the samples in the SRB-300 dataset.

Description
unique clip identifier
path to the Swiss German clip in the audio folder
unique identification of the broadcast station
unique sample identifier
duration of the clip in seconds
start timestamps of the merged utterances in the original audio
end timestamps of the merged utterances in the original audio

Metadata
clip_id
clip_path
sender_id
sample_id
duration_s
starts
ends
original_media_file original audio file, from which the sample was extracted
index
sender
recording_date
gender
speaker
form
type
kind
style_element
language
media_type
text

position of the given sample in the original media file
name of the broadcast station
date on which the sample was recorded from broadcasting
gender(s) of the speaker(s)
kind of speaker (presenter, news reader, reporter, ...)
live, story, report, talk
information, moderation, service, entertainment, ...
information, moderation, traffic, weather, ...
statement, interview, still image, ...
language spoken in the sample
type of broadcasting (usually radio, TV, or social media)
Standard German text (transcript)

8

F. D’Intino, H.-P. Hutter

indoor and outdoor settings (see Appendix A, Tables 10 and 11). Additional
statistical details about the SRB-300 corpus are in Appendix A.

Fig. 2. Distribution of samples over Switzerlands German-speaking area

4.2 Dataset Partitioning

The SRB-300 speech corpus is divided into three datasets: training (76%), val-
idation (10%), and test (14%), as shown in Table 3. The test set also includes
6 hours of audio from 6 radio stations not present in the training or validation
datasets. This allows us to evaluate performance on previously unseen broadcast
stations.

The samples in all datasets are organized according to their original order
in the broadcasts, which allows for long-audio tests by concatenating sequential
samples from the same broadcast. Each sample appears in only one dataset, but
some broadcasts may include samples in different datasets. Additionally, samples
from the same speaker may be present in multiple datasets. This overlap happens
because our metadata lacks unique speaker labels.

Table 3. Sample counts and audio duration of the SRB-300 datasets

Dataset Num. samples Ratio (%) t (h) t rel. (%)
Training
Validation
Test

76
9
15

76
10
14

32714
4078
6315
43107

230
29
44
303

Advancing STT for Low-Resource Real-World Speech

9

Fig. 3. Distribution of the total sample duration per dialect region

5 Experiments

We used different sizes of OpenAI’s Whisper8 model for our fine-tuning experi-
ments. Whisper is an open-source multilingual transformer [29] model designed
for automatic speech recognition, translation, and language detection [24]. It
was trained on an extensive multilingual corpus collected through web crawling.
Thanks to its internal segmentation, Whisper can transcribe audio of arbitrary
length [24].

Although Swiss German is not officially included in Whisper’s training data,
we, along with other researchers, have identified indications of Swiss broadcasts
in the training data [28]. This observation is supported by some hallucinated
outputs generated by the Whisper models. This may help explain why Whisper
models performed surprisingly well in several zero-shot experiments involving
Swiss German transcription [7,26,28].

There are several established approaches for fine-tuning Whisper models
[4,9,13,15,27]. Some specifically focus on low-resource languages [5,8,11,12,18,19,23].
Our fine-tuning process was based on the method described by Sanchit Gandhi [9]
utilizing the Hugging Face implementation of Whisper as part of their transformers-
library [30] at version 4.46 with PyTorch 2.4, along with several Hugging Face

8 https://github.com/openai/whisper

10

F. D’Intino, H.-P. Hutter

libraries to facilitate multi-GPU training. This approach has proven effective for
low-resource languages [12].

We trained the Whisper models using 2 NVIDIA A100 40 GB GPUs for 4
to 12 epochs. We set a batch size of 4, expanded through gradient accumulation
over 32 steps. We utilized an AdamW-optimizer [14]. Furthermore, we applied a
weight decay of 1%. The initial learning rate was set to 5 × e−6 for small and
medium model sizes. However, for the large model sizes, we reduced the initial
learning rate to 1×e−6. To balance the training speed and memory requirements,
we employed gradient checkpointing [3].

Since Swiss German is unavailable as a configurable language in Whisper, we

used the German language tag (DE) instead.

As a reference point, we compared our results against a SOTA XLS-R 1B
model, which was trained on the STT4SG-350 dataset [20]. Our experiments
utilized the Hugging Face implementation for XLS-R.

6 Results

In our comparisons, we calculated WER and BLEU score [17], commonly used
for machine translation tasks, which we consider more relevant for Swiss German
to Standard German transcription (ref. Section 1). For the WER computation,
we utilized the evaluate-library9 of Hugging Face in version 0.4, which internally
uses the jiwer10 implementation. For the BLEU score calculation, we used the
NLTK [2] implementation, version 3.9. Before calculating any metrics, we applied
Whisper’s basic text normalizer to both the output text and the ground truth to
ensure uniformity. The normalizer converts the text to lowercase, for example.
Table 4 summarizes the key findings from our experiments. It indicates that fine-
tuning with the SRB-300 training set led to a reduction in the Whisper zero-shot
WER for all models, with improvements ranging from 19% for Whisper large-
v3 to 33% for Whisper small. Additionally, the BLEU scores increased by 8%
for Whisper large-v3 and up to 40% for Whisper small. The best performance
was achieved with the fine-tuned Whisper large-v3 model (referred to as ZHAW
large-v3), which achieved a WER of 17.1% and a BLEU score of 74.8 on the
SRB-300 test set.

Manual inspection of the transcriptions indicated that they were generally
highly accurate. Most errors were attributed to uncommon named entities, such
as locations, brands, and persons.

Further analysis of the results (see Figure 4) revealed that all dialects in the
training data benefited from the fine-tuning, despite a significant variance in
the quantity of training data available for the different broadcasting stations,
which ranged from less than 2 hours to over 11 hours (ref. Table 9). Also, the
broadcasting stations with no training data improved significantly (marked with
a star in Figure 4).

9 https://github.com/huggingface/evaluate
10 https://github.com/jitsi/jiwer

Advancing STT for Low-Resource Real-World Speech

11

Table 4. Results of different models on the SRB-300 test set using the Hugging Face
implementation of Whisper with a beam width of 2.

Model
Whisper small
ZHAW small
Whisper medium
ZHAW medium
ZHAW medium 2
ZHAW medium 3
Whisper large-v2
ZHAW large-v2
Whisper large-v3
ZHAW large-v3
Whisper large-v3-turbo zero-shot
ZHAW large-v3-turbo on SRB-300
XLS-R-1B
XLS-R-1B

Fine-Tuning
WER BLEU
45.3
zero-shot
37.4
63.2
on SRB-300
25.0
58.1
zero-shot
27.2
70.5
on SRB-300
19.7
50.5
31.4
on STT4SG-350
71.0
on STT4SG-350 and SRB-300 19.4
63.7
23.5
zero-shot
72.3
18.4
on SRB-300
69.1
21.1
zero-shot
on SRB-300
17.1 74.8
61.8
26.5
71.8
18.7
37.5
on STT4SG-350
44.4
60.8
on STT4SG-350 and SRB-300 24.7

Fig. 4. Improvement of mean BLEU score on the SRB-300 test set for all samples
of each broadcasting station for the ZHAW large-v3 model, which was fine-tuned on
SRB-300. The broadcast stations not in the training set are marked with a star. The
y-axis begins at 40 for better readability.

12

F. D’Intino, H.-P. Hutter

Meta’s11 XLS-R 1B model [1], which was specifically fine-tuned on Swiss
German using the STT4SG-350 dataset, demonstrated a significant performance
drop from the reported 14.0% WER and 74.7 BLEU [20] down to 44.4% WER
and 37.5 BLEU score on the SRB-300 test set. This represents a 50% worse
performance in BLEU score compared to the best fine-tuned Whisper model
ZHAW large-v3. Surprisingly, its performance was considerably lower than the
zero-shot performance of Whisper’s medium model, which achieved a WER of
27.2% and a BLEU score of 58.1 on the SRB-300 test set. This underscores the
necessity of utilizing realistic datasets for STT model training.

Additionally, we fine-tuned the XLS-R 1B model on the new SRB-300 dataset.
This resulted in performance improvements comparable to the fine-tuned ZHAW
small model.

In two additional experiments, we aimed to determine how Whisper mod-
els might benefit from fine-tuning using the sentence-level STT4SG-350 dataset
for our specific task. We fine-tuned the medium-sized Whisper model on the
STT4SG-350 dataset and evaluated its performance on the SRB-300 test set
(experiment ZHAW medium 2). However, we observed a 4.2% increase in the
WER and a drop of 7.6 BLEU score compared to the Whisper medium model’s
zero-shot performance. After conducting further fine-tuning with the SRB-300
training set (referred to as ZHAW medium 3), the model’s performance only
slightly improved compared to the Whisper model that had been fine-tuned
solely on the SRB-300 dataset. This suggests that additional fine-tuning data
only helps if recorded in a setting similar to the target application.

The final experiment evaluated the latest Whisper model, large-v3-turbo,12
using the SRB-300 dataset. After fine-tuning with SRB-300, the model performed
similarly to the large-v2 model, but its results were lower than those of the large-
v3 model (see Table 4). One of the key advantages of the large-v3-turbo model
is its inference speed. It operates even faster than the small model size measured
on the SRB-300 test set (see Table 5). It is important to note that the reported
inference times do not account for any preprocessing steps. The experiments
were conducted using two NVIDIA A100 40 GB GPUs, with a batch size of 64
and a beam width of 2.

In a separate experiment, we concatenated consecutive test set samples to
create complete audio broadcasts. The resulting test audio files reached lengths
of up to 1 hour, with an average duration of 21 minutes. Using the ZHAW
medium and ZHAW large-v3 models, we found that the performance is similar
to the results presented in Table 4. This indicates that the findings in Table 4
remain valid even for audio segments considerably longer than those typically
found in scientific datasets.

11 Formerly Facebook
12 https://github.com/openai/whisper/discussions/2363, accessed 19.11.2024

Advancing STT for Low-Resource Real-World Speech

13

Table 5. Inference times on the SRB-300 test set

Model
Whisper small
Whisper medium
Whisper large-v2
Whisper large-v3
Whisper large-v3-turbo

Inference time (min.) Ratio to large-v3 (%)

29
44
47
46
19

63
95
101
100
40

7 Conclusion

The primary contributions of this paper are twofold. First, it introduces a new
realistic speech corpus for Swiss German STT. Unlike many existing datasets
for Swiss German, this new corpus includes long samples of spontaneous speech
recorded in real-world scenarios, such as conversations with various background
noises.

Second, the paper presents a robust STT model for conversational Swiss
German, named ZHAW large-v3. This model performs 50% better on the realistic
SRB-300 test set compared to the best models that have been fine-tuned on the
STT4SG-350 dataset. With a BLEU score of nearly 75, it performs similarly on
the challenging SRB-300 test set as the best-reported model trained and tested
on the scientific STT4SG-350 dataset.

These findings underscore the potential for fine-tuning multilingual STT
models for low-resource languages and emphasize the importance of leveraging
realistic data to enhance these models for real-world applications.

8 Future Work

Although the performance of the ZHAW large-v3 STT model is very promising,
the remaining fallacies often relate to uncommon named entities. These entities
are crucial in professional applications, particularly in legal and medical fields.
Therefore, our future research will focus on fine-tuning our models to improve
their handling of these named entities.

Limitations

The SRB-300 corpus cannot be made publicly available due to data license re-
strictions of the industrial partner. For requests related to the SRB-300 corpus,
please contact our industrial partner.13

Apart from gender, we do not have detailed demographic information about
the speakers of SRB-300. We only know the location of the broadcasting station
and not the origins of the speakers. Local broadcasting stations tend to promote
themselves by employing presenters who speak the local dialect. Thus, we assume

13 Eurospider Information Technology AG, https://eurospider.com/

14

F. D’Intino, H.-P. Hutter

that the dialects used by the speakers roughly correspond to the region where
the station is located. Further uncertainties are detailed in Appendices B and C.

Acknowledgments. We thank our industrial project partners for providing the audio
and metadata for the new SRB-300 corpus. Additionally, we appreciate the assistance
of our students in reviewing and correcting the transcriptions. We also thank all our
colleagues at ETHZ, FHNW, and ZHAW for making their datasets SwissDial, SPC,
SDS-200, and STT4SG-350 available. Innosuisse funded this study under project num-
ber 105.791 IP-ICT.

Disclosure of Interests. The authors have no competing interests to declare that
are relevant to the content of this article.

A Dataset Characteristics

Table 6 lists the distribution of the recorded broadcast types. In Table 7, the
distribution of the speakers’ gender can be seen. Table 8 lists the various speaker
types. Table 9 lists the distribution of broadcasting stations (anonymized). Ta-
ble 10 outlines the distribution of post kind, and Table 11 details the journalistic
forms of the posts. Each table provides the distribution of all data, as well as
that of the training and test sets, respectively.

Table 6. Broadcast type

Media type Total (h) Total (%) Training (h) Training(%) Test (h) Test(%)
radio
149
tv
81

198
106

65
35

68
32

30
14

65
35

Table 7. Available speaker gender information

Speaker Gender Total (h) Total (%) Training (h) Training(%) Test (h) Test(%)
female
female & male
male
unknown

70
49
136
48

55
39
104
32

9
5
19
12

20
11
42
26

23
16
45
16

24
17
45
14

B Personal Data and Offensive Content

The datasets may include personal information, such as the names of speakers
or individuals related to the topic. Since the audio data is sourced from public
broadcasts, we assume that the content is not offensive. Additionally, we have
not received any complaints from the students who listened to the samples while
reviewing the transcriptions.

Advancing STT for Low-Resource Real-World Speech

15

Table 8. Data on speaker types

Total (h) Total (%) Training (h) Training(%) Test (h) Test(%)

0.2
2.6
31.9
0.3
80.6
53.3
14.8
0.6
38.6
0.1
35.2
0.3
45.0

0.1
0.8
10.5
0.1
26.5
17.6
4.9
0.2
12.7
0.0
11.6
0.1
14.8

0.2
2.1
23.9
0.2
58.8
41.8
11.3
0.5
29.8
0.1
27.0
0.2
34.9

0.1
0.9
10.4
0.1
25.5
18.1
4.9
0.2
12.9
0.0
11.7
0.1
15.1

0.0
0.3
5.1
0.1
14.0
6.7
1.5
0.1
5.4
0.0
4.9
0.1
5.8

0.0
0.7
11.6
0.3
31.8
15.3
3.4
0.2
12.2
0.0
11.1
0.2
13.1

Speaker
correspondent
expert
informant
listener
mixed
moderator
multiple moderators
multiple news anchors
news anchor
other
reporter/ journalist
specialist journalist
unknown

C Potential Risks

The corpus was created to reflect diversity, aiming to include all dialect re-
gions, while ensuring a balanced gender ratio. However, children and individuals
over 65 might be significantly underrepresented, as most speakers are broadcast
presenters, likely in their professional years. Thus, older individuals, as well as
younger ones, are primarily represented in conversational contexts (ref. Table 8).
As a result, models trained on this dataset may perform below average for these
demographic groups and individuals with strong, less commonly spoken dialects.

D Transcription Correction Rules

We established guidelines for correcting the automatically generated transcrip-
tions to ensure consistency in the ground truth transcriptions. Additionally, we
created a list where students could enter and search for the correct spelling of
translated words and names (Table 12).

References

1. Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh,
K., Von Platen, P., Saraf, Y., Pino, J., Baevski, A., Conneau, A., Auli,
M.: XLS-R: Self-supervised Cross-lingual Speech Representation Learning at
Scale. In: Interspeech 2022. pp. 2278–2282. ISCA (Sep 2022). https://doi.org/
10.21437/Interspeech.2022-143, https://www.isca-archive.org/interspeech_2022/
babu22_interspeech.html

14 E.g Swiss German does not use past tense.
15 e.g. "ss" instead of "ß", which is used in Germany
16 Whisper generally does not transcribe fillers [15]

16

F. D’Intino, H.-P. Hutter

Table 9. Audio data per broadcast station. IDs that start with 1 correspond to TV
stations, while those starting with 2 correspond to radio stations. Stations 271 to 276
are excluded from the training set.

Broadcast
Station ID
102
106
107
108
109
110
111
112
113
115
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
271
272
273
274
275
276

Total (h) Total (%) Training (h) Training(%) Test (h) Test(%)

9.92
13.41
14.21
11.72
8.81
12.50
9.81
6.98
8.73
9.54
10.17
7.23
8.24
9.54
11.22
11.29
8.30
8.30
7.75
8.48
9.93
8.28
7.11
6.68
2.35
8.09
6.61
8.42
7.20
6.98
12.63
8.87
8.14
3.34
0.04
0.46
1.94
0.20
0.22

3.27
4.42
4.68
3.86
2.90
4.12
3.23
2.30
2.88
3.14
3.35
2.38
2.71
3.14
3.70
3.72
2.73
2.73
2.55
2.79
3.27
2.73
2.34
2.20
0.77
2.66
2.18
2.77
2.37
2.30
4.16
2.92
2.68
1.10
0.01
0.15
0.64
0.07
0.07

7.94
10.73
11.37
9.38
6.15
9.25
7.85
4.01
6.99
7.64
8.14
4.99
5.97
7.64
8.98
9.04
4.55
6.64
6.21
6.91
7.95
5.83
5.69
5.35
1.88
6.47
5.29
6.73
5.76
5.58
10.11
7.10
6.52
0.00
0.00
0.00
0.00
0.00
0.00

3.44
4.65
4.93
4.07
2.67
4.01
3.40
1.74
3.03
3.31
3.53
2.16
2.59
3.31
3.89
3.92
1.97
2.88
2.69
3.00
3.45
2.53
2.47
2.32
0.81
2.81
2.29
2.92
2.50
2.42
4.38
3.08
2.82
0.00
0.00
0.00
0.00
0.00
0.00

0.99
1.34
1.42
1.16
1.88
2.09
0.97
2.47
0.87
0.94
1.01
1.61
1.52
0.94
1.11
1.12
3.19
0.83
0.77
0.78
0.99
1.72
0.70
0.66
0.23
0.80
0.65
0.84
0.71
0.69
1.26
0.88
0.81
3.34
0.04
0.46
1.94
0.20
0.22

2.24
3.03
3.21
2.63
4.27
4.74
2.21
5.59
1.97
2.14
2.28
3.64
3.45
2.13
2.53
2.54
7.22
1.87
1.74
1.76
2.25
3.89
1.59
1.49
0.53
1.81
1.47
1.90
1.60
1.56
2.85
1.99
1.83
7.58
0.09
1.03
4.39
0.45
0.50

Advancing STT for Low-Resource Real-World Speech

17

Post Kind
children’s program
church
cinema/event tips
comedy/sketch
entertainment: other
game moderation
information
live sports
media info.: ext.
moderation
news
parody/satire
prog. info. (own)
ref. to own digi. svc.
service: other
stock exchange
traffic
unknown
weather

Table 10. Kind of post

Total (h) Total (%) Training (h) Training(%) Test (h) Test(%)

0.4
0.1
1.0
0.9
0.2
4.0
120.5
0.3
0.5
57.7
74.4
0.5
1.2
1.7
1.1
0.5
13.1
4.6
20.8

0.1
0.0
0.3
0.3
0.1
1.3
39.7
0.1
0.2
19.0
24.5
0.2
0.4
0.6
0.4
0.2
4.3
1.5
6.9

0.4
0.1
0.7
0.8
0.2
3.4
88.5
0.1
0.3
45.1
57.3
0.4
1.0
1.4
0.9
0.3
10.1
3.5
16.1

0.2
0.0
0.3
0.3
0.1
1.5
38.4
0.0
0.1
19.6
24.8
0.2
0.4
0.6
0.4
0.1
4.4
1.5
7.0

0.0
0.0
0.2
0.0
0.0
0.3
20.9
0.0
0.1
6.7
10.2
0.1
0.1
0.2
0.1
0.0
1.7
0.7
2.8

0.0
0.0
0.4
0.0
0.0
0.7
47.3
0.1
0.3
15.1
23.1
0.2
0.2
0.5
0.3
0.1
3.8
1.5
6.3

Journalistic Form
(live) broadcast
biography/ portrait
commentary
commentary/ column
docu./feat./reportage
explanatory film
headline(s)
interpret./ expl. piece
message
press rev. monothem.
press rev.: var. topics
report
review/ criticism
short report
speaker’s msg./ info
studio convers./ talk
unknown

Table 11. Journalistic form of the post

Total (h) Total (%) Training (h) Training(%) Test (h) Test(%)

0.6
1.5
0.1
0.6
19.3
0.0
2.9
0.2
37.5
0.1
0.2
76.7
0.2
3.2
7.5
40.7
112.3

0.2
0.5
0.0
0.2
6.4
0.0
1.0
0.1
12.4
0.0
0.1
25.3
0.1
1.1
2.5
13.4
37.0

0.5
1.3
0.1
0.4
14.8
0.0
2.2
0.1
28.6
0.1
0.2
58.5
0.1
2.5
6.0
30.7
84.5

0.2
0.6
0.0
0.2
6.4
0.0
1.0
0.0
12.4
0.0
0.1
25.4
0.0
1.1
2.6
13.3
36.6

0.0
0.1
0.0
0.1
2.4
0.0
0.4
0.1
5.5
0.0
0.0
10.5
0.1
0.4
0.8
6.7
17.0

0.1
0.2
0.1
0.1
5.5
0.0
1.0
0.1
12.4
0.0
0.0
23.7
0.1
1.0
1.9
15.1
38.5

18

F. D’Intino, H.-P. Hutter

Table 12. Transcription rules for the manual corrections

What
translations

How
correct if clearly wrong and double-check with the list of
corrected terms
correct if clearly wrong, missing, or too much

words
names, dialect expressions research correct words, and double-check the list of cor-

end of sentence
other punctuation
hyphens
numbers
compounds
tenses14
capitalization
characters
repetitions, filler words16

rected terms
correct if necessary
do not correct
do not correct, do not add any when inserting text
leave as is, if the number (digits or words) itself is correct
do not correct as long as the meaning is correct
do not correct
correct if clearly wrong
use Swiss characters only15
leave as is, as long as it is also contained in the audio

2. Bird, S., Klein, E., Loper, E.: Natural Language Processing with Python. O’Reilly

Media Inc. (Jun 2009), iSBN: 9780596516499

3. Chen, T., Xu, B., Zhang, C., Guestrin, C.: Training Deep Nets with Sublin-
ear Memory Cost (Apr 2016). https://doi.org/10.48550/arXiv.1604.06174, http:
//arxiv.org/abs/1604.06174, arXiv:1604.06174 [cs] version: 2

4. Deschamps-Berger, T.: ASR-whisper-finetuning (Oct 2024), https://github.com/

Theodb/ASR-whisper-finetuning

5. Do, A., Brown, O., Wang, Z., Mathew, N., Liu, Z., Ahmed, J., Yu, C.: Using
fine-tuning and min lookahead beam search to improve Whisper (Sep 2023), http:
//arxiv.org/abs/2309.10299, arXiv:2309.10299 [cs, eess]

6. Dogan-Schönberger, P., Mäder, J., Hofmann, T.: SwissDial: Parallel Multidialectal
Corpus of Spoken Swiss German (Mar 2021), http://arxiv.org/abs/2103.11401,
arXiv:2103.11401 [cs]

7. Dolev, E., Lutz, C., Aepli, N.: Does Whisper Understand Swiss German? An
Automatic, Qualitative, and Human Evaluation. In: Scherrer, Y., Jauhiainen,
T., Ljubešić, N., Zampieri, M., Nakov, P., Tiedemann, J. (eds.) Proceedings
of the Eleventh Workshop on NLP for Similar Languages, Varieties, and Di-
alects (VarDial 2024). pp. 28–40. Association for Computational Linguistics, Mex-
ico City, Mexico (Jun 2024). https://doi.org/10.18653/v1/2024.vardial-1.3, https:
//aclanthology.org/2024.vardial-1.3

8. Ferraz, T.P., Zanon Boito, M., Brun, C., Nikoulina, V.: Multilingual Distilwhis-
per: Efficient Distillation of Multi-Task Speech Models Via Language-Specific
Experts. In: ICASSP 2024 - 2024 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP). pp. 10716–10720 (Apr 2024). https://doi.
org/10.1109/ICASSP48485.2024.10447520, https://ieeexplore.ieee.org/document/
10447520/?arnumber=10447520, iSSN: 2379-190X

9. Gandhi, S.: Fine-Tune Whisper For Multilingual ASR with Hugging Face Trans-

formers (Nov 2022), https://huggingface.co/blog/fine-tune-whisper

10. Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang,
S., Zhang, Z., Wu, Y., Pang, R.: Conformer: Convolution-augmented Trans-
former for Speech Recognition. In: Interspeech 2020. pp. 5036–5040. ISCA (Oct

Advancing STT for Low-Resource Real-World Speech

19

2020). https://doi.org/10.21437/Interspeech.2020-3015, https://www.isca-archive.
org/interspeech_2020/gulati20_interspeech.html

11. Hsu, M.H., Huang, K.P., Lee, H.y.: Meta-Whisper: Speech-Based Meta-ICL for
ASR on Low-Resource Languages (Sep 2024). https://doi.org/10.48550/arXiv.
2409.10429, http://arxiv.org/abs/2409.10429, arXiv:2409.10429 [eess]

12. Liu, Y., Yang, X., Qu, D.: Exploration of Whisper fine-tuning strategies for
low-resource ASR. EURASIP Journal on Audio, Speech, and Music Process-
ing 2024(1), 29 (Jun 2024). https://doi.org/10.1186/s13636-024-00349-3, https:
//doi.org/10.1186/s13636-024-00349-3

13. Lodagala, V.: Fine-tuning and evaluating Whisper models for Automatic Speech
Recognition (May 2023), https://github.com/vasistalodagala/whisper-finetune
14. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: 7th Inter-
national Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net (2019), https://openreview.net/forum?id=
Bkg6RiCqY7

15. Ma, R., Qian, M., Gales, M., Knill, K.M.: Adapting an ASR Foundation Model for
Spoken Language Assessment. In: 9th Workshop on Speech and Language Tech-
nology in Education (SLaTE). pp. 104–108. ISCA (Aug 2023). https://doi.org/
10.21437/SLaTE.2023-20, https://www.isca-archive.org/slate_2023/ma23_slate.
html

16. Paonessa, C., Schraner, Y., Deriu, J., Hürlimann, M., Vogel, M., Cieliebak, M.:
Dialect Transfer for Swiss German Speech Translation. In: Bouamor, H., Pino,
J., Bali, K. (eds.) Findings of the Association for Computational Linguistics:
EMNLP 2023. pp. 15240–15254. Association for Computational Linguistics, Sin-
gapore (Dec 2023). https://doi.org/10.18653/v1/2023.findings-emnlp.1018, https:
//aclanthology.org/2023.findings-emnlp.1018

17. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a Method for Automatic
Evaluation of Machine Translation. In: Isabelle, P., Charniak, E., Lin, D. (eds.)
Proceedings of the 40th Annual Meeting of the Association for Computational
Linguistics. pp. 311–318. Association for Computational Linguistics, Philadelphia,
Pennsylvania, USA (Jul 2002). https://doi.org/10.3115/1073083.1073135, https:
//aclanthology.org/P02-1040

18. Pillai, L.G., Manohar, K., Raju, B.K., Sherly, E.: Multistage Fine-tuning
Strategies
for Automatic Speech Recognition in Low-resource Languages
(Nov 2024). https://doi.org/10.48550/arXiv.2411.04573, http://arxiv.org/abs/
2411.04573, arXiv:2411.04573 [cs]

19. Piñeiro-Martín, A., García-Mateo, C., Docio-Fernandez, L., López-Pérez, M.D.C.,
Rehm, G.: Weighted Cross-entropy for Low-Resource Languages in Multilingual
Speech Recognition. In: Interspeech 2024. pp. 1235–1239. ISCA, Kos, Greece (Sep
2024). https://doi.org/10.21437/Interspeech.2024-734, https://www.isca-archive.
org/interspeech_2024/pineiromartin24_interspeech.html

20. Plüss, M., Deriu, J., Schraner, Y., Paonessa, C., Hartmann, J., Schmidt, L.,
Scheller, C., Hürlimann, M., Samardžić, T., Vogel, M., Cieliebak, M.: STT4SG-
350: A Speech Corpus for All Swiss German Dialect Regions. In: Rogers, A.,
Boyd-Graber, J., Okazaki, N. (eds.) Proceedings of the 61st Annual Meeting
of the Association for Computational Linguistics (Volume 2: Short Papers).
pp. 1763–1772. Association for Computational Linguistics, Toronto, Canada (Jul
2023). https://doi.org/10.18653/v1/2023.acl-short.150, https://aclanthology.org/
2023.acl-short.150

20

F. D’Intino, H.-P. Hutter

21. Plüss, M., Hürlimann, M., Cuny, M., Stöckli, A., Kapotis, N., Hartmann, J., Ulasik,
M.A., Scheller, C., Schraner, Y., Jain, A., Deriu, J., Cieliebak, M., Vogel, M.: SDS-
200: A Swiss German Speech to Standard German Text Corpus. In: Calzolari, N.,
Béchet, F., Blache, P., Choukri, K., Cieri, C., Declerck, T., Goggi, S., Isahara, H.,
Maegaard, B., Mariani, J., Mazo, H., Odijk, J., Piperidis, S. (eds.) Proceedings of
the Thirteenth Language Resources and Evaluation Conference. vol. Proceedings
of the Thirteenth Language Resources and Evaluation Conference, pp. 3250–3256.
European Language Resources Association, Marseille, France (Jun 2022), https:
//aclanthology.org/2022.lrec-1.347

22. Plüss, M., Neukom, L., Scheller, C., Vogel, M.: Swiss Parliaments Corpus, an
Automatically Aligned Swiss German Speech to Standard German Text Corpus.
In: Proceedings of the Swiss Text Analytics Conference 2021. Online (Jun 2021),
https://ceur-ws.org/Vol-2957/paper3.pdf

23. Qian, M., Tang, S., Ma, R., Knill, K.M., Gales, M.J.: Learn and Don’t For-
get: Adding a New Language to ASR Foundation Models. In: Interspeech 2024.
pp. 2544–2548. ISCA (Sep 2024). https://doi.org/10.21437/Interspeech.2024-1045,
https://www.isca-archive.org/interspeech_2024/qian24_interspeech.html

24. Radford, A., Kim, J.W., Xu, T., Brockman, G., Mcleavey, C., Sutskever, I.: Ro-
bust Speech Recognition via Large-Scale Weak Supervision. In: Proceedings of the
40th International Conference on Machine Learning. pp. 28492–28518. ICML’23,
JMLR.org, Honolulu, Hawaii, USA (Jul 2023). https://doi.org/10.5555/3618408.
3619590, https://proceedings.mlr.press/v202/radford23a.html, iSSN: 2640-3498
25. Schraner, Y., Scheller, C., Plüss, M., Vogel, M.: Swiss German Speech to Text
system evaluation (Nov 2022), http://arxiv.org/abs/2207.00412, arXiv:2207.00412
[cs]

26. Sicard, C., Gillioz, V., Pyszkowski, K.: Spaiche: Extending State-of-the-Art ASR
Models to Swiss German Dialects. In: Ghorbel, H., Sokhn, M., Cieliebak, M., Hür-
limann, M., de Salis, E., Guerne, J. (eds.) Proceedings of the 8th edition of the
Swiss Text Analytics Conference. vol. Proceedings of the 8th edition of the Swiss
Text Analytics Conference, pp. 76–83. Association for Computational Linguistics,
Neuchatel, Switzerland (Jun 2023), https://aclanthology.org/2023.swisstext-1.8
27. Srivastav, V.: Faster Whisper Finetuning with LoRA powered by Hugging Face
PEFT (Apr 2023), https://github.com/Vaibhavs10/fast-whisper-finetuning
28. Timmel, V., Paonessa, C., Kakooee, R., Vogel, M., Perruchoud, D.: Fine-
tuning Whisper on Low-Resource Languages
for Real-World Applications
(Dec 2024). https://doi.org/10.48550/arXiv.2412.15726, http://arxiv.org/abs/
2412.15726, arXiv:2412.15726 [cs]

29. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,
Kaiser, L., Polosukhin, I.: Attention is All you Need. In: Advances in Neural In-
formation Processing Systems. vol. 30, pp. 6000–6010. Curran Associates, Inc.
(2017). https://doi.org/10.5555/3295222.3295349, https://papers.nips.cc/paper_
files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html

30. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma,
C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q.,
Rush, A.: Transformers: State-of-the-Art Natural Language Processing. In: Liu, Q.,
Schlangen, D. (eds.) Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations. pp. 38–45. Association for
Computational Linguistics, Online (Oct 2020). https://doi.org/10.18653/v1/2020.
emnlp-demos.6, https://aclanthology.org/2020.emnlp-demos.6

